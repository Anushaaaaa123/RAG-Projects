# -*- coding: utf-8 -*-
"""Chat with PDFS using RAG Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/105wV258ki7Vn9xO3ajEyn6h_6Op64HGV
"""

!pip install pdfplumber

!pip install  camelot-py[cv] sentence-transformers faiss-cpu transformers pandas flask gradio

pip install camelot-py[cv]

!pip install camelot==12.06.29
!pip install camelot-py[cv]

!pip install tabula-py

!apt-get update && apt-get install -y default-jre

import pdfplumber

with pdfplumber.open('/content/Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life - 2017-2018.pdf') as pdf:
    page_2 = pdf.pages[1]  # Page indexing starts at 0
    text = page_2.extract_text()
    print(text)

import re

def extract_unemployment_data(text):
    pattern = r"(\w+ degree): (\d+\.?\d*)%"
    matches = re.findall(pattern, text)
    return {degree: float(rate) for degree, rate in matches}

data = extract_unemployment_data(text)
print(data)

import pdfplumber

def extract_text_from_pdf(pdf_path):
    text_data = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text_data.append(page.extract_text())
    return text_data

pdf_path = "/content/Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life - 2017-2018.pdf"
text_data = extract_text_from_pdf(pdf_path)
print("Sample Text from PDF:", text_data[0])  # Display first page text

!pip install --force-reinstall camelot-py[cv]

from tabula import read_pdf

# Retry extracting tables from the PDF using tabula after installing Java
tabula_tables = read_pdf(pdf_path, pages='all', multiple_tables=True, lattice=True)

# Display the number of tables extracted
len(tabula_tables)

import pandas as pd

# Check if the extracted tables exist and handle the 6th table (page 6)
if len(tabula_tables) > 5:
    # Access the table from page 6
    table_page_6 = tabula_tables[5]

    # Ensure the table is a pandas DataFrame
    if isinstance(table_page_6, pd.DataFrame):
        # Save the table to a CSV file
        table_page_6.to_csv("table_page_6.csv", index=False)
        print("Table from page 6 has been saved as table_page_6.csv.")
    else:
        print("Error: The extracted table on page 6 is not a pandas DataFrame.")
else:
    print("Error: Table on page 6 does not exist in the extracted data.")

def segment_text_into_chunks(text, max_length=500):
    chunks = []
    for page in text:
        words = page.split()
        for i in range(0, len(words), max_length):
            chunks.append(" ".join(words[i:i + max_length]))
    return chunks

text_chunks = segment_text_into_chunks(text_data)
print("Number of Chunks:", len(text_chunks))

!pip install sentence_transformers==3.3.1

# Install the required module
!pip install sentence-transformers

# Retry the code after the installation
from sentence_transformers import SentenceTransformer
import numpy as np

# Load the model
model = SentenceTransformer('all-MiniLM-L6-v2')

def generate_embeddings(chunks):
    embeddings = model.encode(chunks, show_progress_bar=True)
    return embeddings

# Generate embeddings for text_chunks
embeddings = generate_embeddings(text_chunks)

# Save embeddings to a file
np.save("embeddings.npy", embeddings)

!pip install faiss==1.5.3

# Install the required FAISS library
!pip install faiss-cpu

# Retry the code after installation
import faiss
import numpy as np

def create_vector_database(embeddings):
    dimension = embeddings.shape[1]  # Get vector size
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)  # Add vectors to the index
    return index

embeddings = np.load("embeddings.npy")
vector_db = create_vector_database(embeddings)

def query_embedding(query):
    return model.encode([query])[0]

user_query = "What is the unemployment rate for bachelor's degrees?"
query_vec = query_embedding(user_query)

def retrieve_similar_chunks(vector_db, query_vec, k=5):
    distances, indices = vector_db.search(np.array([query_vec]), k)
    return indices

indices = retrieve_similar_chunks(vector_db, query_vec)
print("Relevant Chunks:", [text_chunks[i] for i in indices[0]])

from transformers import pipeline

# Specify the correct Hugging Face model name
generator = pipeline("text-generation", model="distilgpt2")

def generate_response(context, query):
    prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"
    return generator(prompt, max_new_tokens=50, num_return_sequences=1)[0]['generated_text']

retrieved_chunks = " ".join([text_chunks[i] for i in indices[0]])
response = generate_response(retrieved_chunks, user_query)
response

!pip install gradio==5.1.0

import gradio as gr

def answer_query(query):
    query_vec = query_embedding(query)
    indices = retrieve_similar_chunks(vector_db, query_vec)
    retrieved_chunks = " ".join([text_chunks[i] for i in indices[0]])
    return generate_response(retrieved_chunks, query)

interface = gr.Interface(fn=answer_query, inputs="text", outputs="text")
interface.launch()

import gradio as gr

def answer_query(query):
    query_vec = query_embedding(query)
    indices = retrieve_similar_chunks(vector_db, query_vec)
    retrieved_chunks = " ".join([text_chunks[i] for i in indices[0]])
    return generate_response(retrieved_chunks, query)

interface = gr.Interface(fn=answer_query, inputs="text", outputs="text")
interface.launch(share=True)



import pdfplumber
import re
import pandas as pd
from tabula import read_pdf
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
from transformers import pipeline
import gradio as gr

def extract_text_from_pdf(pdf_path):
    text_data = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text_data.append(page.extract_text())
    return text_data

def segment_text_into_chunks(text, max_length=500):
    chunks = []
    for page in text:
        words = page.split()
        for i in range(0, len(words), max_length):
            chunks.append(" ".join(words[i:i + max_length]))
    return chunks

def generate_embeddings(chunks):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(chunks, show_progress_bar=True)
    return embeddings

def create_vector_database(embeddings):
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index

def query_embedding(query):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    return model.encode([query])[0]

def retrieve_similar_chunks(vector_db, query_vec, k=5):
    distances, indices = vector_db.search(np.array([query_vec]), k)
    return indices

def generate_response(context, query):
    generator = pipeline("text-generation", model="distilgpt2")
    prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"
    return generator(prompt, max_new_tokens=50, num_return_sequences=1)[0]['generated_text']

def answer_query(query):
    query_vec = query_embedding(query)
    indices = retrieve_similar_chunks(vector_db, query_vec)
    retrieved_chunks = " ".join([text_chunks[i] for i in indices[0]])
    return generate_response(retrieved_chunks, query)

# Main execution
pdf_path = "/content/Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life - 2017-2018.pdf"
text_data = extract_text_from_pdf(pdf_path)
text_chunks = segment_text_into_chunks(text_data)
embeddings = generate_embeddings(text_chunks)
vector_db = create_vector_database(embeddings)

# Create the Gradio interface
interface = gr.Interface(fn=answer_query, inputs="text", outputs="text")
interface.launch()

